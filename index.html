<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM Energy Benchmark â€” Real GPU Power Measurements for Quantized Inference</title>
  <meta name="description" content="Open benchmark dataset: energy consumption of quantized LLM inference across RTX 5090, RTX 4090D, and T4. NVML power data, perplexity measurements, reproducible scripts." />
  <meta property="og:title" content="LLM Energy Benchmark â€” Quantization Energy-Accuracy Trade-offs" />
  <meta property="og:description" content="Open dataset: real GPU power measurements for INT8, NF4, and FP16 inference across 3 NVIDIA architectures. Includes perplexity data." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://hongping-zh.github.io/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="LLM Energy Benchmark â€” Real GPU Power Data" />
  <meta name="twitter:description" content="Open benchmark: energy cost of INT8/NF4/FP16 quantization across RTX 5090, 4090D, T4. NVML measurements, perplexity included." />
  <meta name="keywords" content="LLM energy benchmark, GPU power measurement, quantization energy cost, INT8 energy, NF4 energy, bitsandbytes, NVML, perplexity, RTX 5090, RTX 4090D, A800, green AI, inference efficiency" />
  <meta name="author" content="Hongping Zhang" />
  <meta name="robots" content="index, follow" />
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸ“Š</text></svg>" />
  <link rel="canonical" href="https://hongping-zh.github.io/" />
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            brand: {
              50: '#eff6ff', 100: '#dbeafe', 200: '#bfdbfe',
              300: '#93c5fd', 400: '#60a5fa', 500: '#3b82f6',
              600: '#2563eb', 700: '#1d4ed8', 800: '#1e40af', 900: '#1e3a5f',
            }
          },
          fontFamily: {
            sans: ['Inter', 'system-ui', '-apple-system', 'sans-serif'],
            mono: ['JetBrains Mono', 'Fira Code', 'monospace'],
          }
        }
      }
    }
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet" />

  <!-- JSON-LD Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Dataset",
    "name": "LLM Energy Benchmark",
    "description": "Open benchmark dataset measuring energy consumption of quantized LLM inference (INT8, NF4, FP16) across multiple NVIDIA GPU architectures, with perplexity accuracy measurements.",
    "url": "https://hongping-zh.github.io/",
    "license": "https://opensource.org/licenses/MIT",
    "creator": {
      "@type": "Person",
      "name": "Hongping Zhang",
      "url": "https://github.com/hongping-zh"
    },
    "distribution": {
      "@type": "DataDownload",
      "encodingFormat": "CSV",
      "contentUrl": "https://github.com/hongping-zh/ecocompute-ai/tree/main/data"
    },
    "measurementTechnique": "NVML power monitoring at 10 Hz sampling rate",
    "variableMeasured": [
      "Energy per 1000 tokens (Joules)",
      "GPU power draw (Watts)",
      "Inference throughput (tokens/second)",
      "Perplexity (PPL)"
    ]
  }
  </script>
  <style>
    html { scroll-behavior: smooth; }
    body { font-family: 'Inter', system-ui, sans-serif; }
    .gradient-text {
      background: linear-gradient(135deg, #3b82f6 0%, #2563eb 50%, #1d4ed8 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    .hero-gradient {
      background: radial-gradient(ellipse 80% 60% at 50% -20%, rgba(59,130,246,0.1) 0%, transparent 70%);
    }
    .data-table { border-collapse: collapse; width: 100%; font-size: 0.8125rem; }
    .data-table th { background: #f8fafc; font-weight: 600; text-align: left; padding: 10px 14px; border-bottom: 2px solid #e2e8f0; color: #475569; font-size: 0.75rem; text-transform: uppercase; letter-spacing: 0.05em; }
    .data-table td { padding: 10px 14px; border-bottom: 1px solid #f1f5f9; }
    .data-table tbody tr:hover { background: #f8fafc; }
    .data-table .num { font-family: 'JetBrains Mono', monospace; font-size: 0.8125rem; }
    .data-table .pos { color: #dc2626; }
    .data-table .neg { color: #16a34a; }
    .gpu-tag { display: inline-flex; align-items: center; gap: 4px; padding: 2px 8px; border-radius: 9999px; font-size: 0.6875rem; font-weight: 600; }
    .gpu-5090 { background: #fef3c7; color: #92400e; }
    .gpu-4090 { background: #dbeafe; color: #1e40af; }
    .gpu-t4 { background: #e0e7ff; color: #3730a3; }
    .gpu-a800 { background: #d1fae5; color: #065f46; }
    .card { background: white; border: 1px solid #e2e8f0; border-radius: 12px; overflow: hidden; }
    .badge { display: inline-flex; align-items: center; gap: 6px; padding: 4px 12px; border-radius: 9999px; font-size: 0.75rem; font-weight: 500; }
  </style>
</head>
<body class="bg-slate-50 text-slate-900 antialiased">

  <!-- ===================== NAV ===================== -->
  <nav class="fixed top-0 left-0 right-0 z-50 bg-white/80 backdrop-blur-lg border-b border-slate-200">
    <div class="max-w-6xl mx-auto flex items-center justify-between px-6 py-3">
      <a href="#" class="flex items-center gap-2 font-bold text-lg">
        <span class="text-xl">ðŸ“Š</span>
        <span class="gradient-text">LLM Energy Benchmark</span>
      </a>
      <div class="flex items-center gap-6">
        <a href="#leaderboard" class="hidden sm:inline text-sm text-slate-500 hover:text-slate-900 transition">Leaderboard</a>
        <a href="#accuracy" class="hidden sm:inline text-sm text-slate-500 hover:text-slate-900 transition">Accuracy</a>
        <a href="#findings" class="hidden sm:inline text-sm text-slate-500 hover:text-slate-900 transition">Key Findings</a>
        <a href="#methodology" class="hidden sm:inline text-sm text-slate-500 hover:text-slate-900 transition">Methodology</a>
        <a href="https://github.com/hongping-zh/ecocompute-ai" target="_blank"
           class="inline-flex items-center gap-2 px-4 py-2 bg-slate-800 text-white text-sm font-semibold rounded-lg hover:bg-slate-900 transition">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-4 h-4" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"/></svg>
          Data &amp; Code
        </a>
      </div>
    </div>
  </nav>

  <!-- ===================== HERO ===================== -->
  <section class="hero-gradient pt-28 pb-12 sm:pt-36 sm:pb-16">
    <div class="max-w-4xl mx-auto px-6 text-center">
      <div class="flex items-center justify-center gap-3 mb-6 flex-wrap">
        <span class="badge bg-blue-50 text-blue-700">Open Data</span>
        <span class="badge bg-green-50 text-green-700">Reproducible</span>
        <span class="badge bg-purple-50 text-purple-700">NVML 10Hz Sampling</span>
        <span class="badge bg-amber-50 text-amber-700">Last updated: Feb 2026</span>
      </div>

      <h1 class="text-3xl sm:text-4xl lg:text-5xl font-extrabold leading-tight tracking-tight">
        How Much Energy Does<br/>
        <span class="gradient-text">LLM Quantization</span> Actually Cost?
      </h1>

      <p class="mt-6 text-base sm:text-lg text-slate-500 max-w-2xl mx-auto leading-relaxed">
        Real GPU power measurements for INT8, NF4, and FP16 inference across 3 NVIDIA architectures.
        Includes perplexity (accuracy) data. All raw data, scripts, and methodology are open source.
      </p>

      <div class="mt-8 flex flex-col sm:flex-row items-center justify-center gap-3">
        <a href="#leaderboard" class="inline-flex items-center gap-2 px-6 py-3 bg-brand-600 text-white font-semibold rounded-lg hover:bg-brand-700 transition">
          View Leaderboard
          <svg xmlns="http://www.w3.org/2000/svg" class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"/></svg>
        </a>
        <a href="https://github.com/hongping-zh/ecocompute-ai/tree/main/data" target="_blank"
           class="inline-flex items-center gap-2 px-6 py-3 text-slate-600 font-medium hover:text-slate-900 border border-slate-300 rounded-lg hover:border-slate-400 transition">
          Download CSV / JSON
        </a>
      </div>
    </div>
  </section>

  <!-- ===================== STATS ===================== -->
  <section class="py-10">
    <div class="max-w-6xl mx-auto px-6">
      <div class="grid grid-cols-2 sm:grid-cols-4 gap-4">
        <div class="card p-5 text-center">
          <div class="text-3xl font-black text-brand-600">3</div>
          <div class="mt-1 text-xs font-medium text-slate-500">GPU Architectures</div>
          <div class="mt-1 text-[10px] text-slate-400">Blackwell / Ada / Turing</div>
        </div>
        <div class="card p-5 text-center">
          <div class="text-3xl font-black text-brand-600">8</div>
          <div class="mt-1 text-xs font-medium text-slate-500">Models Tested</div>
          <div class="mt-1 text-[10px] text-slate-400">1.1B to 7B parameters</div>
        </div>
        <div class="card p-5 text-center">
          <div class="text-3xl font-black text-brand-600">25+</div>
          <div class="mt-1 text-xs font-medium text-slate-500">Configurations</div>
          <div class="mt-1 text-[10px] text-slate-400">FP16 / INT8 / NF4</div>
        </div>
        <div class="card p-5 text-center">
          <div class="text-3xl font-black text-brand-600">n=10</div>
          <div class="mt-1 text-xs font-medium text-slate-500">Runs per Config</div>
          <div class="mt-1 text-[10px] text-slate-400">CV &lt; 3%</div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== LEADERBOARD: RTX 5090 ===================== -->
  <section id="leaderboard" class="py-12">
    <div class="max-w-6xl mx-auto px-6">
      <div class="mb-8">
        <h2 class="text-2xl font-extrabold">Energy Leaderboard</h2>
        <p class="mt-2 text-sm text-slate-500">Energy per 1,000 generated tokens (Joules). Lower is better. Sorted by energy efficiency.</p>
      </div>

      <!-- RTX 5090 -->
      <div class="card mb-6">
        <div class="px-5 py-3 bg-slate-50 border-b border-slate-200 flex items-center justify-between">
          <div class="flex items-center gap-3">
            <span class="gpu-tag gpu-5090">RTX 5090</span>
            <span class="text-sm font-semibold text-slate-700">NVIDIA GeForce RTX 5090 (Blackwell, 32GB)</span>
          </div>
          <span class="text-xs text-slate-400">Batch size = 1, seq_len = 512</span>
        </div>
        <div class="overflow-x-auto">
          <table class="data-table">
            <thead>
              <tr>
                <th>Model</th>
                <th>Params</th>
                <th>Precision</th>
                <th>Energy (J/1k tok)</th>
                <th>Throughput (tok/s)</th>
                <th>Power (W)</th>
                <th>vs FP16</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="font-medium">TinyLlama-1.1B</td>
                <td class="num">1.1B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">1,659</td>
                <td class="num">94.87</td>
                <td class="num">157.5</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">TinyLlama-1.1B</td>
                <td class="num">1.1B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">2,098</td>
                <td class="num">55.79</td>
                <td class="num">117.0</td>
                <td class="num pos">+26.5%</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2-1.5B</td>
                <td class="num">1.5B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">2,411</td>
                <td class="num">71.45</td>
                <td class="num">172.3</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2-1.5B</td>
                <td class="num">1.5B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">3,120</td>
                <td class="num">41.57</td>
                <td class="num">129.8</td>
                <td class="num pos">+29.4%</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2.5-3B</td>
                <td class="num">3B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">3,383</td>
                <td class="num">54.77</td>
                <td class="num">185.6</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2.5-3B</td>
                <td class="num">3B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">3,780</td>
                <td class="num">31.85</td>
                <td class="num">120.5</td>
                <td class="num pos">+11.7%</td>
              </tr>
              <tr style="background:#f0fdf4">
                <td class="font-medium">Qwen2-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">5,509</td>
                <td class="num">70.47</td>
                <td class="num">388.3</td>
                <td class="num">baseline</td>
              </tr>
              <tr style="background:#f0fdf4">
                <td class="font-medium">Qwen2-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">4,878</td>
                <td class="num">41.40</td>
                <td class="num">201.9</td>
                <td class="num neg">-11.4%</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="px-5 py-3 bg-slate-50 border-t border-slate-200 text-xs text-slate-500">
          NF4 saves energy only for 7B+ models on RTX 5090. For smaller models, NF4 <em>increases</em> energy by 12-29%.
        </div>
      </div>

      <!-- RTX 4090D INT8 -->
      <div class="card mb-6">
        <div class="px-5 py-3 bg-slate-50 border-b border-slate-200 flex items-center justify-between">
          <div class="flex items-center gap-3">
            <span class="gpu-tag gpu-4090">RTX 4090D</span>
            <span class="text-sm font-semibold text-slate-700">NVIDIA GeForce RTX 4090D (Ada Lovelace, 24GB)</span>
          </div>
          <span class="text-xs text-slate-400">INT8 quantization, batch size = 1</span>
        </div>
        <div class="overflow-x-auto">
          <table class="data-table">
            <thead>
              <tr>
                <th>Model</th>
                <th>Params</th>
                <th>Precision</th>
                <th>Energy (J/1k tok)</th>
                <th>INT8 Config</th>
                <th>vs FP16</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="font-medium">Yi-1.5-6B</td>
                <td class="num">6B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">4,716</td>
                <td class="text-xs text-slate-400">-</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Yi-1.5-6B</td>
                <td class="num">6B</td>
                <td><span class="badge bg-orange-50 text-orange-700">INT8</span></td>
                <td class="num">6,258</td>
                <td class="text-xs">threshold=6.0 (default)</td>
                <td class="num pos">+32.7%</td>
              </tr>
              <tr>
                <td class="font-medium">Mistral-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">5,661</td>
                <td class="text-xs text-slate-400">-</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Mistral-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-orange-50 text-orange-700">INT8</span></td>
                <td class="num">7,401</td>
                <td class="text-xs">threshold=6.0 (default)</td>
                <td class="num pos">+30.7%</td>
              </tr>
              <tr>
                <td class="font-medium">Phi-3-mini</td>
                <td class="num">3.8B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">3,003</td>
                <td class="text-xs text-slate-400">-</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Phi-3-mini</td>
                <td class="num">3.8B</td>
                <td><span class="badge bg-orange-50 text-orange-700">INT8</span></td>
                <td class="num">3,940</td>
                <td class="text-xs">threshold=6.0 (default)</td>
                <td class="num pos">+31.2%</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2.5-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">5,217</td>
                <td class="text-xs text-slate-400">-</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2.5-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-orange-50 text-orange-700">INT8</span></td>
                <td class="num">6,127</td>
                <td class="text-xs">threshold=6.0 (default)</td>
                <td class="num pos">+17.4%</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="px-5 py-3 bg-slate-50 border-t border-slate-200 text-xs text-slate-500">
          Default INT8 (bitsandbytes <code>llm_int8_threshold=6.0</code>) consistently increases energy by 17-33% due to mixed-precision decomposition overhead. This is the cost of preserving accuracy.
        </div>
      </div>

      <!-- T4 -->
      <div class="card mb-6">
        <div class="px-5 py-3 bg-slate-50 border-b border-slate-200 flex items-center justify-between">
          <div class="flex items-center gap-3">
            <span class="gpu-tag gpu-t4">T4</span>
            <span class="text-sm font-semibold text-slate-700">NVIDIA Tesla T4 (Turing, 16GB)</span>
          </div>
          <span class="text-xs text-slate-400">Batch size = 1, seq_len = 512</span>
        </div>
        <div class="overflow-x-auto">
          <table class="data-table">
            <thead>
              <tr>
                <th>Model</th>
                <th>Params</th>
                <th>Precision</th>
                <th>Energy (J/1k tok)</th>
                <th>Throughput (tok/s)</th>
                <th>Power (W)</th>
                <th>vs FP16</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="font-medium">TinyLlama-1.1B</td>
                <td class="num">1.1B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">4,251</td>
                <td class="num">12.34</td>
                <td class="num">52.5</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">TinyLlama-1.1B</td>
                <td class="num">1.1B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">4,445</td>
                <td class="num">10.87</td>
                <td class="num">48.3</td>
                <td class="num pos">+4.6%</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2-1.5B</td>
                <td class="num">1.5B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">5,732</td>
                <td class="num">9.56</td>
                <td class="num">54.8</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2-1.5B</td>
                <td class="num">1.5B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">5,744</td>
                <td class="num">8.92</td>
                <td class="num">51.2</td>
                <td class="num pos">+0.2%</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2.5-3B</td>
                <td class="num">3B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">11,268</td>
                <td class="num">5.23</td>
                <td class="num">58.9</td>
                <td class="num">baseline</td>
              </tr>
              <tr>
                <td class="font-medium">Qwen2.5-3B</td>
                <td class="num">3B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">11,112</td>
                <td class="num">5.01</td>
                <td class="num">55.7</td>
                <td class="num neg">-1.4%</td>
              </tr>
              <tr style="background:#f0fdf4">
                <td class="font-medium">Qwen2-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="num">21,723</td>
                <td class="num">2.87</td>
                <td class="num">62.3</td>
                <td class="num">baseline</td>
              </tr>
              <tr style="background:#f0fdf4">
                <td class="font-medium">Qwen2-7B</td>
                <td class="num">7B</td>
                <td><span class="badge bg-purple-50 text-purple-700">NF4</span></td>
                <td class="num">18,735</td>
                <td class="num">3.12</td>
                <td class="num">58.5</td>
                <td class="num neg">-13.8%</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="px-5 py-3 bg-slate-50 border-t border-slate-200 text-xs text-slate-500">
          On T4, NF4 energy savings begin at ~3B parameters. Effect is more pronounced for 7B models (-13.8%).
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== ACCURACY (PPL) ===================== -->
  <section id="accuracy" class="py-12 bg-white">
    <div class="max-w-6xl mx-auto px-6">
      <div class="mb-8">
        <h2 class="text-2xl font-extrabold">Accuracy Assessment</h2>
        <p class="mt-2 text-sm text-slate-500">Perplexity (PPL) on WikiText-2 test set. Lower is better. Measures accuracy cost of quantization.</p>
      </div>

      <div class="card">
        <div class="px-5 py-3 bg-slate-50 border-b border-slate-200 flex items-center justify-between">
          <div class="flex items-center gap-3">
            <span class="gpu-tag gpu-4090">RTX 4090D</span>
            <span class="text-sm font-semibold text-slate-700">Yi-1.5-6B on WikiText-2 (50 samples, max_len=512)</span>
          </div>
          <span class="text-xs text-slate-400">Feb 2026</span>
        </div>
        <div class="overflow-x-auto">
          <table class="data-table">
            <thead>
              <tr>
                <th>Configuration</th>
                <th>Precision</th>
                <th>INT8 Threshold</th>
                <th>Perplexity</th>
                <th>PPL vs FP16</th>
                <th>Energy vs FP16</th>
                <th>Memory (GB)</th>
                <th>Assessment</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="font-medium">FP16 (baseline)</td>
                <td><span class="badge bg-blue-50 text-blue-700">FP16</span></td>
                <td class="text-xs text-slate-400">-</td>
                <td class="num font-semibold">11.16</td>
                <td class="num">-</td>
                <td class="num">-</td>
                <td class="num">12.12</td>
                <td><span class="badge bg-green-50 text-green-700">Baseline</span></td>
              </tr>
              <tr>
                <td class="font-medium">INT8 Default</td>
                <td><span class="badge bg-orange-50 text-orange-700">INT8</span></td>
                <td class="num">6.0</td>
                <td class="num font-semibold">11.20</td>
                <td class="num pos">+0.33%</td>
                <td class="num pos">+32.7%</td>
                <td class="num">6.70</td>
                <td><span class="badge bg-green-50 text-green-700">Negligible loss</span></td>
              </tr>
              <tr>
                <td class="font-medium">INT8 Pure</td>
                <td><span class="badge bg-orange-50 text-orange-700">INT8</span></td>
                <td class="num">0.0</td>
                <td class="num font-semibold">14.00</td>
                <td class="num pos">+25.38%</td>
                <td class="num neg">-3.1%</td>
                <td class="num">6.70</td>
                <td><span class="badge bg-red-50 text-red-700">Significant loss</span></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="px-5 py-3 bg-slate-50 border-t border-slate-200 text-xs text-slate-500">
          Default INT8 preserves accuracy (+0.33% PPL) but costs +32.7% more energy. Pure INT8 (threshold=0.0) saves only 3.1% energy while losing 25% accuracy. <strong>This validates the bitsandbytes default design.</strong>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== KEY FINDINGS ===================== -->
  <section id="findings" class="py-12">
    <div class="max-w-6xl mx-auto px-6">
      <div class="mb-8">
        <h2 class="text-2xl font-extrabold">Key Findings</h2>
      </div>

      <div class="grid md:grid-cols-3 gap-5">
        <div class="card p-6">
          <div class="text-2xl mb-3">1</div>
          <h3 class="font-bold text-base mb-2">INT8 Costs More Energy</h3>
          <p class="text-sm text-slate-500 leading-relaxed">
            Default <code class="text-xs bg-slate-100 px-1 py-0.5 rounded">LLM.int8()</code> increases energy consumption by <strong>17-33%</strong> vs FP16 on consumer GPUs due to mixed-precision decomposition overhead. This is the cost of preserving accuracy.
          </p>
        </div>
        <div class="card p-6">
          <div class="text-2xl mb-3">2</div>
          <h3 class="font-bold text-base mb-2">NF4 Crossover at ~5B Params</h3>
          <p class="text-sm text-slate-500 leading-relaxed">
            NF4 quantization saves energy only for models with <strong>&ge;5B parameters</strong>. For smaller models, the dequantization compute cost exceeds the memory bandwidth savings, <em>increasing</em> energy by 5-29%.
          </p>
        </div>
        <div class="card p-6">
          <div class="text-2xl mb-3">3</div>
          <h3 class="font-bold text-base mb-2">Pure INT8 Harms Accuracy</h3>
          <p class="text-sm text-slate-500 leading-relaxed">
            Disabling mixed-precision decomposition (<code class="text-xs bg-slate-100 px-1 py-0.5 rounded">threshold=0.0</code>) causes <strong>+25% perplexity degradation</strong> while saving only 3% energy. The trade-off is not justified.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== METHODOLOGY ===================== -->
  <section id="methodology" class="py-12 bg-white">
    <div class="max-w-6xl mx-auto px-6">
      <div class="mb-8">
        <h2 class="text-2xl font-extrabold">Methodology</h2>
        <p class="mt-2 text-sm text-slate-500">Full reproducibility details. All scripts available in the repository.</p>
      </div>

      <div class="grid md:grid-cols-2 gap-5">
        <div class="card p-6">
          <h3 class="font-bold text-sm text-slate-700 mb-3 uppercase tracking-wide">Hardware Platforms</h3>
          <table class="text-sm w-full">
            <tbody>
              <tr class="border-b border-slate-100">
                <td class="py-2 font-medium"><span class="gpu-tag gpu-5090">RTX 5090</span></td>
                <td class="py-2 text-slate-500">Blackwell, 32GB GDDR7, 575W TDP</td>
              </tr>
              <tr class="border-b border-slate-100">
                <td class="py-2 font-medium"><span class="gpu-tag gpu-4090">RTX 4090D</span></td>
                <td class="py-2 text-slate-500">Ada Lovelace, 24GB GDDR6X, 425W TDP</td>
              </tr>
              <tr class="border-b border-slate-100">
                <td class="py-2 font-medium"><span class="gpu-tag gpu-t4">T4</span></td>
                <td class="py-2 text-slate-500">Turing, 16GB GDDR6, 70W TDP</td>
              </tr>
              <tr>
                <td class="py-2 font-medium"><span class="gpu-tag gpu-a800">A800</span></td>
                <td class="py-2 text-slate-500">Ampere, 80GB HBM2e, 300W TDP</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="card p-6">
          <h3 class="font-bold text-sm text-slate-700 mb-3 uppercase tracking-wide">Measurement Protocol</h3>
          <ul class="space-y-2 text-sm text-slate-500">
            <li class="flex items-start gap-2"><span class="text-brand-500 mt-0.5 font-bold shrink-0">-</span>Power sampling: <strong>NVML at 10 Hz</strong></li>
            <li class="flex items-start gap-2"><span class="text-brand-500 mt-0.5 font-bold shrink-0">-</span>Repetitions: <strong>n=10</strong> per configuration</li>
            <li class="flex items-start gap-2"><span class="text-brand-500 mt-0.5 font-bold shrink-0">-</span>Coefficient of variation: <strong>&lt; 3%</strong></li>
            <li class="flex items-start gap-2"><span class="text-brand-500 mt-0.5 font-bold shrink-0">-</span>Warmup: 3 runs discarded before measurement</li>
            <li class="flex items-start gap-2"><span class="text-brand-500 mt-0.5 font-bold shrink-0">-</span>Perplexity: WikiText-2 test split, cross-entropy loss</li>
            <li class="flex items-start gap-2"><span class="text-brand-500 mt-0.5 font-bold shrink-0">-</span>Software: PyTorch 2.4+, bitsandbytes, transformers</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== COMMUNITY ===================== -->
  <section class="py-12">
    <div class="max-w-6xl mx-auto px-6">
      <div class="mb-8">
        <h2 class="text-2xl font-extrabold">Community & Contributions</h2>
      </div>
      <div class="grid md:grid-cols-2 gap-5">
        <a href="https://github.com/bitsandbytes-foundation/bitsandbytes/pull/1882" target="_blank" class="card p-6 hover:border-brand-300 transition group">
          <div class="flex items-center gap-2 mb-2">
            <span class="badge bg-green-50 text-green-700">Open PR</span>
            <span class="text-xs text-slate-400">#1882</span>
          </div>
          <h3 class="font-bold text-sm group-hover:text-brand-600 transition">Quantization Performance Guide for bitsandbytes</h3>
          <p class="mt-1 text-xs text-slate-500">Documentation PR adding energy-accuracy trade-off analysis to official bitsandbytes docs.</p>
        </a>
        <a href="https://github.com/bitsandbytes-foundation/bitsandbytes/issues/1867" target="_blank" class="card p-6 hover:border-brand-300 transition group">
          <div class="flex items-center gap-2 mb-2">
            <span class="badge bg-blue-50 text-blue-700">Discussion</span>
            <span class="text-xs text-slate-400">#1867</span>
          </div>
          <h3 class="font-bold text-sm group-hover:text-brand-600 transition">INT8 Energy Overhead Discussion with Tim Dettmers</h3>
          <p class="mt-1 text-xs text-slate-500">Discussion with the bitsandbytes core author on the energy implications of mixed-precision decomposition.</p>
        </a>
      </div>
    </div>
  </section>

  <!-- ===================== CITE ===================== -->
  <section class="py-12 bg-white">
    <div class="max-w-6xl mx-auto px-6">
      <div class="mb-6">
        <h2 class="text-2xl font-extrabold">Cite This Benchmark</h2>
      </div>
      <div class="card p-5">
        <pre class="text-xs font-mono text-slate-600 overflow-x-auto leading-relaxed">@misc{zhang2026llmenergy,
  author    = {Zhang, Hongping},
  title     = {LLM Energy Benchmark: Real GPU Power Measurements for Quantized Inference},
  year      = {2026},
  url       = {https://github.com/hongping-zh/ecocompute-ai},
  note      = {NVML power monitoring, 3 GPU architectures, includes perplexity data}
}</pre>
      </div>
    </div>
  </section>

  <!-- ===================== FOOTER ===================== -->
  <footer class="border-t border-slate-200 py-8 bg-slate-50">
    <div class="max-w-6xl mx-auto px-6">
      <div class="flex flex-col sm:flex-row items-center justify-between gap-4">
        <div class="flex items-center gap-2 text-sm text-slate-400">
          <span class="text-lg">ðŸ“Š</span>
          <span>LLM Energy Benchmark</span>
          <span>&middot;</span>
          <span>by <a href="https://github.com/hongping-zh" target="_blank" class="text-slate-600 hover:underline">Hongping Zhang</a></span>
        </div>
        <div class="flex items-center gap-5 text-sm text-slate-400">
          <a href="https://github.com/hongping-zh/ecocompute-ai" target="_blank" class="hover:text-slate-700 transition">Repository</a>
          <a href="https://github.com/hongping-zh/ecocompute-ai/tree/main/data" target="_blank" class="hover:text-slate-700 transition">Raw Data</a>
          <a href="https://github.com/hongping-zh/ecocompute-ai/tree/main/benchmarks" target="_blank" class="hover:text-slate-700 transition">Scripts</a>
          <a href="./privacy.html" class="hover:text-slate-700 transition">Privacy</a>
        </div>
      </div>
      <p class="text-center text-xs text-slate-300 mt-6">
        "Measure, don't assume. Reproduce, don't trust. Share, don't hoard."
      </p>
    </div>
  </footer>

</body>
</html>
